# Retry Operations

Retries enable workflows to automatically recover from transient technical failures by reattempting failed operations after a delay.

## Error Types: Technical vs Business

It's important to distinguish between:

- **Technical errors** – Infrastructure-level problems like network timeouts or service unavailability. These are typically transient and can be retried.
- **Business errors** – Domain-specific conditions like invalid input or failed validations. These require explicit handling and **should not be retried**.

Use [error handling](/docs/operations/handle-errors) for managing business errors.

## How Retry Works

When a workflow is resumed (e.g. via `wakeup` on `WorkflowInstance`) and the operation fails:

- **Without retry**: the error is propagated to the caller.
- **With retry**: the error is swallowed, and a `KnockerUpper` schedules a future wakeup based on retry logic.

## Retry Strategies

### 1. Simple Retry with Fixed Delay

```scala 3
val operation = WIO.runIO( /* some operation that might fail */ )
val withRetry = operation.retryIn {
  case _: NetworkException => Duration.ofSeconds(30)
  case _: TimeoutException => Duration.ofMinutes(1)
}

```

### 2. Advanced Retry with Custom Logic

```scala 3
val operation = WIO.runIO( /* some operation that might fail */ )
val withRetry = operation.retry { (error, state, now) =>
  error match {
    case _: NetworkException =>
      IO.pure(Some(now.plus(Duration.ofSeconds(30))))
    case _: TimeoutException =>
      IO.pure(Some(now.plus(Duration.ofMinutes(1))))
    case _                   =>
      IO.pure(None) // Don't retry other errors
  }
}

```

## Caveats

### Retries Are Stateless

Currently, Workflow-level retries are **stateless**—they don’t track attempt counts,
elapsed time or any other information about executed retries.
This means you can't directly express rules like:

- "Retry at most 5 times"
- "Stop after 1 day"

To support such logic, you can:
- Use custom persistent state to track retry metadata and query it in the retry handler.
- Manually clear the scheduled wakeup time via the `KnockerUpper` to stop future retries.

### Choose the Right Layer

Use workflow-level retries for:
- Recovering from long-lived or external failures (e.g., service unavailability)
- Retry schedules spanning **minutes to hours or days**

For short-lived retries (e.g., retrying within milliseconds or seconds), prefer handling them directly inside the `IO` operation using libraries like [`cats-retry`](https://github.com/cats-effect/cats-retry).